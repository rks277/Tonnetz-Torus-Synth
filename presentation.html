<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>How It Works - Torus Tonnetz Synthesizer</title>
  <!-- MathJax CDN for LaTeX rendering -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
  <style>
    body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      background: #f9f9f9;
      padding: 20px;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3 {
      color: #222;
    }
    h1 {
      margin-bottom: 20px;
      font-size: 2.8em;
      text-align: center;
    }
    .section {
      margin-bottom: 40px;
    }
    pre {
      background: #eee;
      padding: 10px;
      border: 1px solid #ccc;
      overflow-x: auto;
    }
    .equation {
      background: #fff;
      border-left: 3px solid #007ACC;
      padding: 10px;
      margin: 10px 0;
      font-family: Consolas, monospace;
      font-size: 1.1em;
    }
    .image-placeholder {
      display: block;
      width: 100%;
      max-width: 600px;
      margin: 20px auto;
      border: 1px solid #ccc;
      background: #ddd;
      padding: 10px;
      text-align: center;
      font-style: italic;
      color: #555;
    }
    a {
      color: #007ACC;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <h1>Torus Tonnetz Synthesizer: How It Works</h1>

  <div class="section" id="reaction-diffusion">
    <h2>1. Gray–Scott Reaction–Diffusion Simulation</h2>
    <p>
      The simulation is based on a variant of the Gray–Scott model. Two chemical concentrations, <em>u</em> and <em>v</em>, evolve over time on a 200×200 grid. Their evolution is governed by the following PDEs:
    </p>
    <p class="equation">
      $$\frac{\partial u}{\partial t} = D_u\,\nabla^2 u - u\,v^2 + F\,(1 - u)$$
    </p>
    <p class="equation">
      $$\frac{\partial v}{\partial t} = D_v\,\nabla^2 v + u\,v^2 - (F+k)\,v$$
    </p>
    <p>
      Here:
    </p>
    <ul>
      <li><em>D<sub>u</sub></em> and <em>D<sub>v</sub></em> are the diffusion rates for u and v.</li>
      <li><em>F</em> is the feed rate, and <em>k</em> is the kill rate.</li>
      <li>The Laplacian operator \( \nabla^2 \) is numerically approximated for each cell.</li>
    </ul>
    <p>
      <strong>Reaction–Diffusion:</strong>
      I initially implemented this in pygame as that was the most straightforward tool, but I ultimately rewrote everything in JavaScript using Three.js since that has support for nice 3D graphics and it would be really straighforward to make a simple website.
    </p>
    <img src="figs/pythonRD.png" alt="Reaction–Diffusion Diagram Placeholder" class="image-placeholder">
  </div>

  <div class="section" id="tonnetz">
    <h2>2. Tonnetz Grid & Sound Interpolation</h2>
    <p>
      The Tonnetz is a musical network that shows relationships between pitches. Our grid is a 6×12 array where each cell contains a musical note (e.g., “C”, “G”, “D”, etc.) arranged in a pattern that reflects harmonic relationships.
    </p>
    <p>
      The grid is drawn on an overlay canvas with connecting lines (both horizontal and diagonal) to form the complete network. When a user clicks or drags on the canvas, the system finds the three closest grid points.
    </p>
    <p>
      In <strong>single note mode</strong>, their frequencies are interpolated using an inverse‑distance weighted average. The interpolated frequency \( f_{interp} \) is computed as:
    </p>
    <p class="equation">
      $$f_{interp} = \frac{\sum_{i=1}^{3} \frac{1}{d_i+\epsilon} f_i}{\sum_{i=1}^{3} \frac{1}{d_i+\epsilon}}$$
    </p>
    <p>
      Here, \( f_i \) are the candidate frequencies and \( d_i \) are their distances from the click point (with \( \epsilon \) as a small constant for stability). In <strong>harmony mode</strong>, all three candidate notes are played simultaneously, with volumes scaled by the corresponding weights.
    </p>
    <p>
      <strong>Tonnetz Diagram:</strong> taken from /https://thatsmaths.com/2017/12/28/doughnuts-and-tonnetze/
    </p>
    <img src="figs/tonnetz.png" alt="Tonnetz Diagram Placeholder" class="image-placeholder">
  </div>

  <div class="section" id="sound-synthesis">
    <h2>3. Sound Synthesis and Instrument Options</h2>
    <p>
      I have very little knowledge about music, so I had ChatGPT write functions to create various sounds which I felt sounded cool. The result uses the Web Audio API for real-time sound synthesis. Multiple instrument options are available:
    </p>
    <ul>
      <li><strong>Basic Synth:</strong> A single oscillator with selectable waveforms (sine, square, triangle, sawtooth).</li>
      <li><strong>FM Synth:</strong> Uses a modulator oscillator to create complex timbres.</li>
      <li><strong>Bell:</strong> Produces a bell–like tone with a fast attack and long decay.</li>
      <li><strong>Organ:</strong> Simulates an organ by combining two detuned square waves.</li>
    </ul>
    <p>
      The instrument is chosen from a dropdown in the top–right. In single note mode, one note (from the weighted interpolation) is played. In harmony mode, all three candidate notes are played simultaneously with gains scaled according to their inverse–distance weights.
    </p>
    <p>
      <strong>Instrument Options:</strong>
    </p>
    <img src="figs/instruments.png" alt="Synth Diagram Placeholder" class="image-placeholder">
  </div>

  <div class="section" id="threejs">
    <h2>4. Three.js 3D Integration</h2>
    <p>
      The live reaction–diffusion simulation is also used as a texture for a 3D torus in a Three.js scene. The torus rotates slowly, adding a dynamic visual element that complements the instrument's sound generation.
    </p>
    <p>
      <strong>3D View Screenshot:</strong>
    </p>
    <img src="figs/three.png" alt="3D View Placeholder" class="image-placeholder">
    <pre class="code">
// Simplified Three.js initialization:
function initThree() {
  scene = new THREE.Scene();
  camera = new THREE.PerspectiveCamera(75, window.innerWidth/window.innerHeight, 0.1, 1000);
  camera.position.z = 3;
  
  renderer = new THREE.WebGLRenderer({ antialias: true });
  renderer.setSize(window.innerWidth, window.innerHeight);
  document.body.appendChild(renderer.domElement);
  
  const light = new THREE.DirectionalLight(0xffffff, 1);
  light.position.set(3,3,3);
  scene.add(light);
  
  texture = new THREE.CanvasTexture(simCanvas);
  texture.wrapS = texture.wrapT = THREE.RepeatWrapping;
  texture.repeat.set(2,2);
  
  const geometry = new THREE.TorusGeometry(1, 0.4, 128, 256);
  const material = new THREE.MeshStandardMaterial({
    map: texture,
    displacementMap: texture,
    displacementScale: 0.2,
    metalness: 0.2,
    roughness: 0.7
  });
  torus = new THREE.Mesh(geometry, material);
  scene.add(torus);
}
    </pre>
  </div>

  <div class="section" id="interaction">
    <h2>5. User Interaction</h2>
    <p>
      The system is fairly interactive. Users can click or drag on the overlay canvas to disturb the reaction–diffusion simulation and trigger sound synthesis.
    </p>
    <ul>
      <li>The program finds the three closest Tonnetz grid points to the click position.</li>
      <li>It computes a weighted average frequency using the equation above for monophonic mode, or uses the individual frequencies with weighted gains in harmony mode.</li>
      <li>The selected instrument(s) are triggered to generate sound.</li>
      <li>A localized disturbance is applied to the simulation at the weighted average position.</li>
      <li>A visualization shows the candidate points and the computed average on a small canvas.</li>
    </ul>
    <pre class="code">
// Example event handling excerpt:
overlayCanvas.addEventListener("mousedown", e => {
  isDrawing = true;
  drawOnOverlay(e);
  handleInterpolatedNote(e);
});
overlayCanvas.addEventListener("mousemove", e => {
  if (isDrawing) {
    drawOnOverlay(e);
    handleInterpolatedNote(e);
  }
});
    </pre>
  </div>

  <hr>
  <p style="text-align: center; color: #666;">© 2025 Torus Tonnetz Synthesizer</p>
</body>
</html>
