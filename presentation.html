<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>How Everything Works - Reaction Diffusion + Tonnetz Synthesis</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      padding: 20px;
      background: #f9f9f9;
      line-height: 1.6;
    }
    h1, h2, h3 {
      color: #333;
    }
    pre {
      background: #eee;
      padding: 10px;
      border: 1px solid #ccc;
      overflow-x: auto;
    }
    .section {
      margin-bottom: 40px;
    }
    .code {
      font-size: 0.9em;
    }
    a {
      color: #0066cc;
    }
  </style>
</head>
<body>
  <h1>Reaction Diffusion + Tonnetz Synthesis System: How It Works</h1>
  <p>This document explains how the various parts of the project work together to create an interactive system that blends a reaction–diffusion simulation, a Tonnetz grid visualization, sound interpolation, and multiple instrument synthesis options. The system also provides a 3D view using Three.js.</p>

  <div class="section" id="rd-simulation">
    <h2>1. Reaction–Diffusion Simulation</h2>
    <p>
      The reaction–diffusion simulation is implemented using a canvas (named <code>simCanvas</code>) that runs a Gray–Scott–type simulation. The grid is internally represented by two 2D arrays, <code>u</code> and <code>v</code>, of size 200×200.
    </p>
    <p>
      Each frame, the simulation calculates the Laplacian of each cell and updates the state using the Gray–Scott equations. The reaction parameters <code>DU</code>, <code>DV</code>, <code>F</code>, and <code>K</code> are controlled via the user interface at the bottom left.
    </p>
    <p>
      The simulation is drawn four times onto an overlay canvas to create a 400×400 view, which is then used as a texture in a Three.js 3D scene.
    </p>
    <pre class="code">
function updateRD() {
  // Read parameters from UI.
  params.DU = parseFloat(document.getElementById("DU").value);
  params.DV = parseFloat(document.getElementById("DV").value);
  params.F  = parseFloat(document.getElementById("F").value);
  params.K  = parseFloat(document.getElementById("K").value);
  // Update each cell using Gray–Scott equations.
  for (let y = 0; y < HEIGHT; y++) {
    for (let x = 0; x < WIDTH; x++) {
      const a = u[y][x], b = v[y][x];
      const ua = lap(u, x, y);
      const vb = lap(v, x, y);
      const uvv = a * b * b;
      u_next[y][x] = a + (params.DU * ua - uvv + params.F * (1 - a)) * dt;
      v_next[y][x] = b + (params.DV * vb + uvv - (params.F + params.K) * b) * dt;
    }
  }
  // Swap the arrays.
  [u, u_next] = [u_next, u];
  [v, v_next] = [v_next, v];
}
    </pre>
  </div>

  <div class="section" id="tonnetz-grid">
    <h2>2. Tonnetz Grid and Interpolation</h2>
    <p>
      The Tonnetz is a network originally used in music theory to display the relationships between pitches (or chords). In this project the Tonnetz grid is hardcoded as a 6×12 array. Each cell contains a note (e.g. "C", "G", "D", etc.) and its (x, y) position is computed based on a horizontal spacing and a vertical margin.
    </p>
    <p>
      Lines connecting the grid points are drawn on the overlay canvas. Horizontal lines connect adjacent points in a row. Diagonal connections are drawn between rows so that you see the characteristic interlocking network.
    </p>
    <pre class="code">
// Example: Grid and connection drawing (simplified excerpt)
function drawHardcodedTonnetz(ctx) {
  ctx.save();
  ctx.strokeStyle = "rgba(255,255,255,0.3)";
  // Draw horizontal lines between adjacent points.
  for (let row = 0; row < numRows; row++) {
    for (let col = 0; col < numCols - 1; col++) {
      const pt1 = gridPoints[row][col];
      const pt2 = gridPoints[row][col+1];
      ctx.beginPath();
      ctx.moveTo(pt1.x, pt1.y);
      ctx.lineTo(pt2.x, pt2.y);
      ctx.stroke();
    }
  }
  // Draw diagonal connections (logic differs on odd/even rows).
  // ... (see complete code for full details)
  ctx.restore();
}
    </pre>
    <p>
      When a user clicks or drags on the overlay canvas, the system collects the three closest Tonnetz points. In monophonic mode, these points are used to compute an interpolated frequency (weighted by the reciprocal of the distance) for a single note. In harmony mode, all three candidate notes are played simultaneously with volumes weighted by their computed weights.
    </p>
  </div>

  <div class="section" id="sound-synthesis">
    <h2>3. Sound Synthesis and Instrument Options</h2>
    <p>
      The project uses the Web Audio API to generate sounds. There are multiple synthesis methods:
    </p>
    <ul>
      <li><strong>Basic Synth:</strong> Plays a simple oscillator. Several waveform options are available (sine, square, triangle, and sawtooth).</li>
      <li><strong>FM Synth:</strong> Uses frequency modulation by creating a modulator oscillator whose output modulates the carrier oscillator's frequency.</li>
      <li><strong>Bell:</strong> Generates a bell-like tone with a fast attack and long exponential decay.</li>
      <li><strong>Organ:</strong> Simulates an organ-like sound by playing two detuned square waves.</li>
    </ul>
    <p>
      The instrument is selected from the dropdown in the top–right. In the code, the corresponding function is called when a note is triggered. For example:
    </p>
    <pre class="code">
function playBasicTone(freq, type, vol = 0.2) {
  let osc = audioCtx.createOscillator();
  let gain = audioCtx.createGain();
  osc.type = type; // "sine", "square", "triangle", or "sawtooth"
  osc.frequency.setValueAtTime(freq, audioCtx.currentTime);
  gain.gain.setValueAtTime(vol, audioCtx.currentTime);
  osc.connect(gain);
  gain.connect(audioCtx.destination);
  osc.start();
  osc.stop(audioCtx.currentTime + 0.5);
}
    </pre>
    <p>
      For harmony mode, the top three candidate notes are played concurrently with their gains scaled according to their inverse–distance weights.
    </p>
  </div>

  <div class="section" id="threejs">
    <h2>4. Three.js 3D View</h2>
    <p>
      A Three.js scene is created to provide a 3D view of the reaction–diffusion texture. The simulation canvas (<code>simCanvas</code>) is used as a texture for a torus. The torus is animated (rotating slightly) while the texture is updated each frame.
    </p>
    <pre class="code">
function initThree() {
  scene = new THREE.Scene();
  camera = new THREE.PerspectiveCamera(75, window.innerWidth/window.innerHeight, 0.1, 1000);
  camera.position.z = 3;
  renderer = new THREE.WebGLRenderer({ antialias: true });
  renderer.setSize(window.innerWidth, window.innerHeight);
  document.body.appendChild(renderer.domElement);
  
  const light = new THREE.DirectionalLight(0xffffff, 1);
  light.position.set(3, 3, 3);
  scene.add(light);
  
  texture = new THREE.CanvasTexture(simCanvas);
  texture.wrapS = texture.wrapT = THREE.RepeatWrapping;
  texture.repeat.set(2,2);
  
  const geometry = new THREE.TorusGeometry(1, 0.4, 128, 256);
  const material = new THREE.MeshStandardMaterial({
    map: texture,
    displacementMap: texture,
    displacementScale: 0.2,
    metalness: 0.2,
    roughness: 0.7
  });
  torus = new THREE.Mesh(geometry, material);
  scene.add(torus);
}
    </pre>
  </div>

  <div class="section" id="interaction">
    <h2>5. User Interaction</h2>
    <p>
      The system is highly interactive. Users can click or drag on the overlay canvas (top right) to both inject disturbances into the reaction–diffusion simulation and trigger sound events.
    </p>
    <p>
      When a user clicks:
    </p>
    <ul>
      <li>The code finds the three closest grid points on the Tonnetz.</li>
      <li>It computes the weighted average frequency (for monophonic mode) or uses each candidate’s frequency (for harmony mode).</li>
      <li>The selected instrument(s) are played accordingly.</li>
      <li>The reaction–diffusion simulation is disturbed at the weighted average position.</li>
      <li>A small visualization (in the bottom right) shows candidate points and the computed average.</li>
    </ul>
    <pre class="code">
// Excerpt from event handling code:
overlayCanvas.addEventListener("mousedown", e => {
  isDrawing = true;
  drawOnOverlay(e);
  handleInterpolatedNote(e);
});
overlayCanvas.addEventListener("mousemove", e => {
  if (isDrawing) {
    drawOnOverlay(e);
    handleInterpolatedNote(e);
  }
});
    </pre>
  </div>

  <div class="section" id="conclusion">
    <h2>Conclusion</h2>
    <p>
      This project combines several creative coding techniques:
    </p>
    <ul>
      <li>A reaction–diffusion simulation that provides evolving organic textures.</li>
      <li>A Tonnetz grid that visually represents musical pitch relationships.</li>
      <li>Sound synthesis using the Web Audio API with multiple instrument options and both monophonic and harmonic playback modes.</li>
      <li>Integration of a Three.js 3D scene where the simulation texture is mapped onto a rotating torus.</li>
    </ul>
    <p>
      Together, these components create a unique, multisensory interactive system where visuals and sound are driven by both deterministic simulation and user interaction.
    </p>
    <p>
      For further customization, you can adjust synthesis parameters, grid dimensions, simulation parameters, or add additional instruments and effects. Enjoy exploring and making this project your own!
    </p>
  </div>

  <hr>
  <p style="text-align: center; color: #666;">© 2025 Reaction Diffusion + Tonnetz Synthesis System</p>
</body>
</html>
